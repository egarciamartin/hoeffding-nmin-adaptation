\section{treedata File Reference}
\label{treedata}\index{treedata@{treedata}}


\subsection{Detailed Description}
Creates a synthetic data set by sampling from a randomly generated Decision\-Tree. 

This program creates a synthetic binary tree and then uses it to label data which can then be used to evaluate learning algorithms. It has been used to evaluate the {\bf vfdt} system.

The synthetic tree is generated starting from a single node as follows. A leaf is selected and is either split on one of the active attributes (each discrete attribute is used at most once on a path from the root to a leaf) or it is pruned. The probability of pruning is set by the {\em prune\-Percent\/} parameter but is 0 if the leaf is not below {\em first\-Prune\-Level\/} and is 1 if the leaf is below {\em max\-Prune\-Level\/}. If the attribute selected for a split is continuous a threshold is generated uniformly in the range 0-1 except that Tree Data ensures that the chosen threshold is not redundant with an earlier split.

Once the tree's structure is created (all leaves have been pruned) a class label is randomly assigned to each leaf and then redundant subtrees (where every leaf has the same classification) are pruned.

Data (training, testing, and pruning) is generated by creating an example and setting its attributes with uniform probability. The tree is used to label the data, and then the class and discrete lables are resampled with uniform probability (without replacement) as specified by the {\em noise\/} parameter and continuous attributes are changed by sampling from a gaussian with mean of their current value and standard deviation of {\em noise\/}.

Using the same {\em concept\-Seed\/} (along with the same other parameter) results in the same concept being created. The same {\em seed\/} results in data being generated the same (so experiments are easily repeatable).

This program also outputs some additional statistics into the {\em stem\/}.stats file.

\subsubsection*{Arguments}

\begin{itemize}
\item -f 'stem name'\begin{itemize}
\item (default DF)\end{itemize}
\item -discrete 'number of discrete attributes'\begin{itemize}
\item (default 100)\end{itemize}
\item -continuous 'number of continous attributes'\begin{itemize}
\item (default 10)\end{itemize}
\item -classes 'number of classes'\begin{itemize}
\item (default 2)\end{itemize}
\item -train 'size of training set'\begin{itemize}
\item (default 50000)\end{itemize}
\item -test 'size of testing set'\begin{itemize}
\item (default 50000)\end{itemize}
\item -prune 'size of prune set'\begin{itemize}
\item (default 50000)\end{itemize}
\item -stdout\begin{itemize}
\item Output the trainset to stdout (default to 'stem'.data)\end{itemize}
\item -noise 'percentage noise (as float (eg: 10.2 is 10.2\%)'\begin{itemize}
\item (default 0)\end{itemize}
\item -prune\-Percent '\%of nodes to prune at each level'\begin{itemize}
\item (default is 25, that's 25\%)\end{itemize}
\item -first\-Prune\-Level 'don't prune nodes before this level'\begin{itemize}
\item (default is 3)\end{itemize}
\item -max\-Prune\-Level 'prune every node after this level'\begin{itemize}
\item (default is 18)\end{itemize}
\item -concept\-Seed 'the multiplier for the concept seed'\begin{itemize}
\item (default 100)\end{itemize}
\item -seed 'random seed'\begin{itemize}
\item (default to random)\end{itemize}
\item -v\begin{itemize}
\item Increase the message level\end{itemize}
\item -h\begin{itemize}
\item Run with this argument to get a list of arguments and their meanings.\end{itemize}
\end{itemize}


\subsubsection*{Example}

{\tt }

{\tt treedata -discrete 10 -continuous 0 -noise 15 -concept\-Seed 21 -seed 1234 -prune\-Percent 15 -train 100 -test 100 -prune 100}

Creates 100 training, 100 testing, and 100 pruning examples from a concept tree made with 15\% chance of pruning each node past level 3 and 100\% chance of pruning past level 18. 15\% noise is added to the data. Finally, the same data set will be produced by multiple calls to the function because of the {\em seed\/} arguments.

