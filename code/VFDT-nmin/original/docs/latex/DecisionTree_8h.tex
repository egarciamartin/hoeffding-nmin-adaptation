\section{Decision\-Tree.h File Reference}
\label{DecisionTree_8h}\index{DecisionTree.h@{DecisionTree.h}}


\subsection{Detailed Description}
A Decision Tree Structure. 

This is the interface for creating, using, printing, \& serializing Decision Trees. A decision tree is a recursive structure. Each internal node partitions the data based on the values of an attribute. Each leaf contains a prediction for the distinguished target attribute. For a more detailed discussion see chapter 3 of {\tt Tom Mitchell's book on machine learning}.

Note that all the Decision\-Trees created with an Example\-Spec maintain a pointer to the it; you shouldn't free or modify the Example\-Spec until you are done with all the Decision\-Trees referencing it.

\begin{Desc}
\item[{\bf Wish List}]A standard in memory decision tree induction algorithm. Maybe the best starting point would be the {\bf decisionstump} learner. 

This isn't the right place for this wish, but it would be nice to have a Rule\-Set structure similar to this Decision\-Tree structure \end{Desc}


\subsection*{Data Structures}
\begin{CompactItemize}
\item 
struct {\bf \_\-Decision\-Tree\_\-}
\begin{CompactList}\small\item\em ADT for working with decision trees. \item\end{CompactList}\end{CompactItemize}
\subsection*{Typedefs}
\begin{CompactItemize}
\item 
typedef {\bf \_\-Decision\-Tree\_\-} {\bf Decision\-Tree}
\begin{CompactList}\small\item\em ADT for working with decision trees. \item\end{CompactList}\item 
typedef {\bf \_\-Decision\-Tree\_\-} $\ast$ {\bf Decision\-Tree\-Ptr}
\begin{CompactList}\small\item\em ADT for working with decision trees. \item\end{CompactList}\end{CompactItemize}
\subsection*{Functions}
\begin{CompactItemize}
\item 
{\bf Decision\-Tree\-Ptr} {\bf Decision\-Tree\-New} ({\bf Example\-Spec\-Ptr} spec)
\begin{CompactList}\small\item\em Creates a new decision tree node. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Free} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Frees the memory associated with the decision tree and all of its children. \item\end{CompactList}\item 
{\bf Decision\-Tree\-Ptr} {\bf Decision\-Tree\-Clone} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Recursively creates a copy of dt and returns it. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Is\-Leaf} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns 1 if dt is a leaf node and 0 otherwise. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Is\-Tree\-Growing} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns 1 if dt is a growing node has any children which are, and 0 otherwise. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Is\-Node\-Growing} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns 1 if dt is a growing node and 0 otherwise. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Get\-Class} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns the index of the class which dt predicts. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Set\-Class} ({\bf Decision\-Tree\-Ptr} dt, int the\-Class)
\begin{CompactList}\small\item\em Sets dt's class prediction to the\-Class. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Add\-To\-Class\-Distribution} ({\bf Decision\-Tree\-Ptr} dt, {\bf Example\-Ptr} e)
\begin{CompactList}\small\item\em Put the example in the class distribution at the node. \item\end{CompactList}\item 
float {\bf Decision\-Tree\-Get\-Class\-Prob} ({\bf Decision\-Tree\-Ptr} dt, int the\-Class)
\begin{CompactList}\small\item\em Returns the probability of the class. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Set\-Class\-Prob} ({\bf Decision\-Tree\-Ptr} dt, int the\-Class, float prob)
\begin{CompactList}\small\item\em Sets the probability of the class. \item\end{CompactList}\item 
float {\bf Decision\-Tree\-Get\-Class\-Distribution\-Sample\-Count} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns the number of samples added to the node's distribution. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Zero\-Class\-Distribution} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Sets the nodes distribution to zeros. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Set\-Type\-Leaf} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Changes dt into a leaf node without changing its class prediction. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Set\-Type\-Growing} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Changes dt into a growing node. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Split\-On\-Discrete\-Attribute} ({\bf Decision\-Tree\-Ptr} dt, int att\-Num)
\begin{CompactList}\small\item\em Changes dt into a discrete split. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Split\-On\-Continuous\-Attribute} ({\bf Decision\-Tree\-Ptr} dt, int att\-Num, float threshold)
\begin{CompactList}\small\item\em Changes dt into a continuous split. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Get\-Child\-Count} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns a count of the direct decendants of dt. \item\end{CompactList}\item 
{\bf Decision\-Tree\-Ptr} {\bf Decision\-Tree\-Get\-Child} ({\bf Decision\-Tree\-Ptr} dt, int index)
\begin{CompactList}\small\item\em Returns one of the direct decendants of dt. \item\end{CompactList}\item 
{\bf Decision\-Tree\-Ptr} {\bf Decision\-Tree\-One\-Step\-Classify} ({\bf Decision\-Tree\-Ptr} dt, {\bf Example\-Ptr} e)
\begin{CompactList}\small\item\em Does one step of classifing e with dt. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Classify} ({\bf Decision\-Tree\-Ptr} dt, {\bf Example\-Ptr} e)
\begin{CompactList}\small\item\em Uses dt to classify e and returns the index of the predicted class. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Gather\-Growing\-Nodes} ({\bf Decision\-Tree\-Ptr} dt, Void\-AList\-Ptr list)
\begin{CompactList}\small\item\em Searches dt and appends all its growing nodes to the passed list. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Gather\-Leaves} ({\bf Decision\-Tree\-Ptr} dt, Void\-AList\-Ptr list)
\begin{CompactList}\small\item\em Searches dt and appends all its leaf nodes to the passed list. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Count\-Nodes} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Returns a count of the number of nodes (of any type) in dt. \item\end{CompactList}\item 
int {\bf Decision\-Tree\-Get\-Most\-Common\-Class} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Return the index of the class that is predicted most commonly by leaf nodes in dt. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Set\-Growing\-Data} ({\bf Decision\-Tree\-Ptr} dt, void $\ast$data)
\begin{CompactList}\small\item\em Each decision tree node has a pointer reserved for your use. \item\end{CompactList}\item 
void $\ast$ {\bf Decision\-Tree\-Get\-Growing\-Data} ({\bf Decision\-Tree\-Ptr} dt)
\begin{CompactList}\small\item\em Each decision tree node has a pointer reserved for your use. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Print} ({\bf Decision\-Tree\-Ptr} dt, FILE $\ast$out)
\begin{CompactList}\small\item\em Prints the decision tree to the passed file. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Print\-Stats} ({\bf Decision\-Tree\-Ptr} dt, FILE $\ast$out)
\begin{CompactList}\small\item\em Prints counts of leaves at each level of the tree. \item\end{CompactList}\item 
{\bf Decision\-Tree\-Ptr} {\bf Decision\-Tree\-Read\-C45} (FILE $\ast$in, {\bf Example\-Spec\-Ptr} spec)
\begin{CompactList}\small\item\em Attempts to read a decision tree from the passed file. \item\end{CompactList}\item 
{\bf Decision\-Tree\-Ptr} {\bf Decision\-Tree\-Read} (FILE $\ast$in, {\bf Example\-Spec\-Ptr} spec)
\begin{CompactList}\small\item\em Attempts to read a decision tree from the passed file. \item\end{CompactList}\item 
void {\bf Decision\-Tree\-Write} ({\bf Decision\-Tree\-Ptr} dt, FILE $\ast$out)
\begin{CompactList}\small\item\em Writes the decision tree to the passed file. \item\end{CompactList}\end{CompactItemize}


\subsection{Typedef Documentation}
\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTree@{DecisionTree}}
\index{DecisionTree@{DecisionTree}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}typedef struct {\bf \_\-Decision\-Tree\_\-}  {\bf Decision\-Tree}}\label{DecisionTree_8h_a0}


ADT for working with decision trees. 

See {\bf Decision\-Tree.h} for more detail. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreePtr@{DecisionTreePtr}}
\index{DecisionTreePtr@{DecisionTreePtr}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}typedef struct {\bf \_\-Decision\-Tree\_\-} $\ast$ {\bf Decision\-Tree\-Ptr}}\label{DecisionTree_8h_a1}


ADT for working with decision trees. 

See {\bf Decision\-Tree.h} for more detail. 

\subsection{Function Documentation}
\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeAddToClassDistribution@{DecisionTreeAddToClassDistribution}}
\index{DecisionTreeAddToClassDistribution@{DecisionTreeAddToClassDistribution}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Add\-To\-Class\-Distribution ({\bf Decision\-Tree\-Ptr} {\em dt}, {\bf Example\-Ptr} {\em e})}\label{DecisionTree_8h_a14}


Put the example in the class distribution at the node. 

Updates the class distribution at the node with the class of the example. This does not make recursive calls, so you should use Decision\-Tree\-One\-Step\-Classify and add it everywhere until you get to a leaf (if that is what you intend). \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeClassify@{DecisionTreeClassify}}
\index{DecisionTreeClassify@{DecisionTreeClassify}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Classify ({\bf Decision\-Tree\-Ptr} {\em dt}, {\bf Example\-Ptr} {\em e})}\label{DecisionTree_8h_a26}


Uses dt to classify e and returns the index of the predicted class. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeClone@{DecisionTreeClone}}
\index{DecisionTreeClone@{DecisionTreeClone}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf Decision\-Tree\-Ptr} Decision\-Tree\-Clone ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a8}


Recursively creates a copy of dt and returns it. 

This function copies the user data pointers, but doesn't copy the data they point to. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeCountNodes@{DecisionTreeCountNodes}}
\index{DecisionTreeCountNodes@{DecisionTreeCountNodes}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Count\-Nodes ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a29}


Returns a count of the number of nodes (of any type) in dt. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeFree@{DecisionTreeFree}}
\index{DecisionTreeFree@{DecisionTreeFree}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Free ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a7}


Frees the memory associated with the decision tree and all of its children. 

This function doesn't do anything with user growing data you may have attached using Decision\-Tree\-Set\-Growing\-Data; you must deal with that before calling this function. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGatherGrowingNodes@{DecisionTreeGatherGrowingNodes}}
\index{DecisionTreeGatherGrowingNodes@{DecisionTreeGatherGrowingNodes}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Gather\-Growing\-Nodes ({\bf Decision\-Tree\-Ptr} {\em dt}, Void\-AList\-Ptr {\em list})}\label{DecisionTree_8h_a27}


Searches dt and appends all its growing nodes to the passed list. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGatherLeaves@{DecisionTreeGatherLeaves}}
\index{DecisionTreeGatherLeaves@{DecisionTreeGatherLeaves}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Gather\-Leaves ({\bf Decision\-Tree\-Ptr} {\em dt}, Void\-AList\-Ptr {\em list})}\label{DecisionTree_8h_a28}


Searches dt and appends all its leaf nodes to the passed list. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetChild@{DecisionTreeGetChild}}
\index{DecisionTreeGetChild@{DecisionTreeGetChild}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf Decision\-Tree\-Ptr} Decision\-Tree\-Get\-Child ({\bf Decision\-Tree\-Ptr} {\em dt}, int {\em index})}\label{DecisionTree_8h_a24}


Returns one of the direct decendants of dt. 

Index should be between 0 and Decision\-Tree\-Get\-Child\-Count(dt) - 1. For nodes that split on continuous attributes use index 0 for the left child ($<$) and index 1 for the right child ($>$=). \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetChildCount@{DecisionTreeGetChildCount}}
\index{DecisionTreeGetChildCount@{DecisionTreeGetChildCount}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Get\-Child\-Count ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a23}


Returns a count of the direct decendants of dt. 

That is, return a count of all the nodes that you can reach from dt by taking one step towards the leaves. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetClass@{DecisionTreeGetClass}}
\index{DecisionTreeGetClass@{DecisionTreeGetClass}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Get\-Class ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a12}


Returns the index of the class which dt predicts. 

This makes the most sense if dt is a Leaf node, but may be useful at other times as well. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetClassDistributionSampleCount@{DecisionTreeGetClassDistributionSampleCount}}
\index{DecisionTreeGetClassDistributionSampleCount@{DecisionTreeGetClassDistributionSampleCount}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}float Decision\-Tree\-Get\-Class\-Distribution\-Sample\-Count ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a17}


Returns the number of samples added to the node's distribution. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetClassProb@{DecisionTreeGetClassProb}}
\index{DecisionTreeGetClassProb@{DecisionTreeGetClassProb}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}float Decision\-Tree\-Get\-Class\-Prob ({\bf Decision\-Tree\-Ptr} {\em dt}, int {\em the\-Class})}\label{DecisionTree_8h_a15}


Returns the probability of the class. 

Returns what portion of the examples that were added to the class distribution at this node have the associated class. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetGrowingData@{DecisionTreeGetGrowingData}}
\index{DecisionTreeGetGrowingData@{DecisionTreeGetGrowingData}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void$\ast$ Decision\-Tree\-Get\-Growing\-Data ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a32}


Each decision tree node has a pointer reserved for your use. 

Use the Get\-Growing\-Data function to access the value of the pointer. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeGetMostCommonClass@{DecisionTreeGetMostCommonClass}}
\index{DecisionTreeGetMostCommonClass@{DecisionTreeGetMostCommonClass}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Get\-Most\-Common\-Class ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a30}


Return the index of the class that is predicted most commonly by leaf nodes in dt. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeIsLeaf@{DecisionTreeIsLeaf}}
\index{DecisionTreeIsLeaf@{DecisionTreeIsLeaf}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Is\-Leaf ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a9}


Returns 1 if dt is a leaf node and 0 otherwise. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeIsNodeGrowing@{DecisionTreeIsNodeGrowing}}
\index{DecisionTreeIsNodeGrowing@{DecisionTreeIsNodeGrowing}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Is\-Node\-Growing ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a11}


Returns 1 if dt is a growing node and 0 otherwise. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeIsTreeGrowing@{DecisionTreeIsTreeGrowing}}
\index{DecisionTreeIsTreeGrowing@{DecisionTreeIsTreeGrowing}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int Decision\-Tree\-Is\-Tree\-Growing ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a10}


Returns 1 if dt is a growing node has any children which are, and 0 otherwise. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeNew@{DecisionTreeNew}}
\index{DecisionTreeNew@{DecisionTreeNew}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf Decision\-Tree\-Ptr} Decision\-Tree\-New ({\bf Example\-Spec\-Ptr} {\em spec})}\label{DecisionTree_8h_a6}


Creates a new decision tree node. 

You should use the accessor methods to initialize it and attach it to an existing Decision\-Tree as needed. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeOneStepClassify@{DecisionTreeOneStepClassify}}
\index{DecisionTreeOneStepClassify@{DecisionTreeOneStepClassify}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf Decision\-Tree\-Ptr} Decision\-Tree\-One\-Step\-Classify ({\bf Decision\-Tree\-Ptr} {\em dt}, {\bf Example\-Ptr} {\em e})}\label{DecisionTree_8h_a25}


Does one step of classifing e with dt. 

Returns the direct decendant of dt corresponding to the correct value of dt's test attribute. If dt is a leaf or growing node this function will return dt. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreePrint@{DecisionTreePrint}}
\index{DecisionTreePrint@{DecisionTreePrint}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Print ({\bf Decision\-Tree\-Ptr} {\em dt}, FILE $\ast$ {\em out})}\label{DecisionTree_8h_a33}


Prints the decision tree to the passed file. 

FILE $\ast$ should be opened for writing. The decision tree will be written so as to be understandable by humans. Your mileage may vary.

Note that you could pass STDOUT to the function to write a decision tree to the console. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreePrintStats@{DecisionTreePrintStats}}
\index{DecisionTreePrintStats@{DecisionTreePrintStats}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Print\-Stats ({\bf Decision\-Tree\-Ptr} {\em dt}, FILE $\ast$ {\em out})}\label{DecisionTree_8h_a34}


Prints counts of leaves at each level of the tree. 

The passed FILE $\ast$ should be opened for writing. Note that you could pass STDOUT to the function to write the stats to the console. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeRead@{DecisionTreeRead}}
\index{DecisionTreeRead@{DecisionTreeRead}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf Decision\-Tree\-Ptr} Decision\-Tree\-Read (FILE $\ast$ {\em in}, {\bf Example\-Spec\-Ptr} {\em spec})}\label{DecisionTree_8h_a37}


Attempts to read a decision tree from the passed file. 

FILE $\ast$ should be opened for reading. Attaches the Example\-Spec to the read decision tree.

This function allocates memory which should be freed by calling Decision\-Tree\-Free. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeReadC45@{DecisionTreeReadC45}}
\index{DecisionTreeReadC45@{DecisionTreeReadC45}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf Decision\-Tree\-Ptr} Decision\-Tree\-Read\-C45 (FILE $\ast$ {\em in}, {\bf Example\-Spec\-Ptr} {\em spec})}\label{DecisionTree_8h_a35}


Attempts to read a decision tree from the passed file. 

FILE $\ast$ should be opened for reading. The file, in, should contain a decision tree written in C4.5's binary format, not the pretty-printed text format. A run of C4.5 with its default arguments will produce 2 such files, stem.tree and stem.unpruned.

This function handles leaves, continuous splits, and discrete splits and will not be able to read trees built with C4.5's subsetting options. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSetClass@{DecisionTreeSetClass}}
\index{DecisionTreeSetClass@{DecisionTreeSetClass}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Set\-Class ({\bf Decision\-Tree\-Ptr} {\em dt}, int {\em the\-Class})}\label{DecisionTree_8h_a13}


Sets dt's class prediction to the\-Class. 

Does not change dt's type to leaf node. This might be useful for anytime algorithms where a growing node needs to contain a reasonable prediction at all times. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSetClassProb@{DecisionTreeSetClassProb}}
\index{DecisionTreeSetClassProb@{DecisionTreeSetClassProb}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Set\-Class\-Prob ({\bf Decision\-Tree\-Ptr} {\em dt}, int {\em the\-Class}, float {\em prob})}\label{DecisionTree_8h_a16}


Sets the probability of the class. 

Changes the probability of the class without changing the sample count (unless the sample count was zero in which case it is set to 1). \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSetGrowingData@{DecisionTreeSetGrowingData}}
\index{DecisionTreeSetGrowingData@{DecisionTreeSetGrowingData}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Set\-Growing\-Data ({\bf Decision\-Tree\-Ptr} {\em dt}, void $\ast$ {\em data})}\label{DecisionTree_8h_a31}


Each decision tree node has a pointer reserved for your use. 

Use the Set\-Growing\-Data function to change the value of the pointer. You can set the pointer to anything you like (for example, to store sufficient statistics on growing nodes), but remember that you are responsible to manage any memory that it points to. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSetTypeGrowing@{DecisionTreeSetTypeGrowing}}
\index{DecisionTreeSetTypeGrowing@{DecisionTreeSetTypeGrowing}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Set\-Type\-Growing ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a20}


Changes dt into a growing node. 

\index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSetTypeLeaf@{DecisionTreeSetTypeLeaf}}
\index{DecisionTreeSetTypeLeaf@{DecisionTreeSetTypeLeaf}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Set\-Type\-Leaf ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a19}


Changes dt into a leaf node without changing its class prediction. 

If dt is not a growing node this function also frees all of dt's children. Remember that you are responsible for anything stored in any of dt's children's growing pointers and you should clean up these pointers before calling this function. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSplitOnContinuousAttribute@{DecisionTreeSplitOnContinuousAttribute}}
\index{DecisionTreeSplitOnContinuousAttribute@{DecisionTreeSplitOnContinuousAttribute}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Split\-On\-Continuous\-Attribute ({\bf Decision\-Tree\-Ptr} {\em dt}, int {\em att\-Num}, float {\em threshold})}\label{DecisionTree_8h_a22}


Changes dt into a continuous split. 

The new node splits on a threshold on a continuous attribute and adds children to dt for values of att\-Num $<$ and $>$= the threshold. The created children start as growing nodes. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeSplitOnDiscreteAttribute@{DecisionTreeSplitOnDiscreteAttribute}}
\index{DecisionTreeSplitOnDiscreteAttribute@{DecisionTreeSplitOnDiscreteAttribute}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Split\-On\-Discrete\-Attribute ({\bf Decision\-Tree\-Ptr} {\em dt}, int {\em att\-Num})}\label{DecisionTree_8h_a21}


Changes dt into a discrete split. 

The new node splits on the values of a discrete attribute and adds one child to dt for each value of attribute att\-Num. The created children start as growing nodes. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeWrite@{DecisionTreeWrite}}
\index{DecisionTreeWrite@{DecisionTreeWrite}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Write ({\bf Decision\-Tree\-Ptr} {\em dt}, FILE $\ast$ {\em out})}\label{DecisionTree_8h_a38}


Writes the decision tree to the passed file. 

FILE $\ast$ should be opened for writing. The decision tree will be written in a binary format suitable to be read by Decision\-Tree\-Read, but this function ignores any growing data that you've associated with dt -- if you need to save growing data you will need to serialize it some other way.

Note that you could pass STDOUT to the function to write an example to the console. \index{DecisionTree.h@{Decision\-Tree.h}!DecisionTreeZeroClassDistribution@{DecisionTreeZeroClassDistribution}}
\index{DecisionTreeZeroClassDistribution@{DecisionTreeZeroClassDistribution}!DecisionTree.h@{Decision\-Tree.h}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void Decision\-Tree\-Zero\-Class\-Distribution ({\bf Decision\-Tree\-Ptr} {\em dt})}\label{DecisionTree_8h_a18}


Sets the nodes distribution to zeros. 

